# Predicting popularity of New York Times blog articles 
## 15.071x - The Analytics Edge (Spring 2015)

This Kaggle competition focused on predicting the popularity of [New York Times blog articles](https://www.kaggle.com/c/15-071x-the-analytics-edge-competition-spring-2015). The purpose of such a model is to help news organizations prioritize the order in which blog articles appear on their websites.  The model I submitted predicted popularity with 91% accuracy and was ranked 60th out of 2923 entries, corresponding to a top 3% finish.

My approach in this competition was to use a simple, well tuned model with a robust cross validation strategy. I was also careful not to make decisions based on the public leaderboard score, i.e., while I made sure my public leaderboard score was consistent with my internal cross validation score, I didn’t make decisions based on the magnitude of my public leaderboard score alone. This approach ensured I did not overfit my models to that public leaderboard, which many folks did.

Indeed, many of the top performers on the public leaderboard performed very poorly on the private leaderboard, as their obsession with optimizing their models for a high public leaderboard score resulted in leaking test information into their model. Therefore, one key learning in this competition was the importance of a robust cross validation strategy that avoided inadvertent data leakage and selection bias.

My model is nothing too fancy – just a random forest regression model with a fairly standard set of features and a robust cross validation strategy.  I implemented my cross validation strategy using a simple switch, which allowed me to easily toggle between “internal” assessment mode and “external” assessment mode.  The internal assessment mode splits Kaggle’s training data into train and test sets and quantifies performance by reporting AUC, i.e., the percentage of the time the model predicted popularity correctly.  The external assessment mode prepares the Kaggle submission by first re-training the model using all Kaggle’s training data and then making predictions using Kaggle’s test data.
